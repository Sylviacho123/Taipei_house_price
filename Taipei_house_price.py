
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd

# åˆå§‹æ¨¡å‹è©•ä¼°è¡¨æ ¼
dct = {'æ¨¡å‹': [], 'ç´°ç¯€':[], 'RMSE (test)':[],
       'R2 (train)':[], 'adj. R2 (train)':[],
       'R2 (test)':[], 'adj. R2 (test)':[]}
df_eval = pd.DataFrame(dct)

# è®€å–è³‡æ–™
df = pd.read_csv('Taipei_house.csv')

# one-hot encoding è¡Œæ”¿å€
df = pd.get_dummies(df, columns=['è¡Œæ”¿å€'])

# è™•ç†è»Šä½é¡åˆ¥ï¼šæœ‰ç‚º1ï¼Œç„¡ç‚º0
df['è»Šä½é¡åˆ¥'] = [0 if x=='ç„¡' else 1 for x in df['è»Šä½é¡åˆ¥']]

# å»ºç«‹æƒ¡æ„æŠ¬åƒ¹æ¬„ä½
df['æ˜¯å¦æƒ¡æ„æŠ¬åƒ¹'] = 0
district_cols = [col for col in df.columns if col.startswith('è¡Œæ”¿å€_')]
for col in district_cols:
    idx = df[col] == 1
    avg = df.loc[idx, 'ç¸½åƒ¹'].mean()
    std = df.loc[idx, 'ç¸½åƒ¹'].std()
    df.loc[idx & (df['ç¸½åƒ¹'] > avg + 2 * std), 'æ˜¯å¦æƒ¡æ„æŠ¬åƒ¹'] = 1

# Adjusted R2 å‡½æ•¸
def adj_R2(r2, n, k):
    return r2 - (k-1)/(n-k) * (1 - r2)

# æ¨¡å‹è©•ä¼°
from sklearn.metrics import mean_squared_error
def measurement(model, X_train, X_test):
    y_pred = model.predict(X_test)

    rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)), 0)
    r2_train = round(model.score(X_train, y_train), 4)
    adj_r2_train = round(adj_R2(r2_train, X_train.shape[0], X_train.shape[1]), 4)
    r2_test = round(model.score(X_test, y_test), 4)
    adj_r2_test = round(adj_R2(r2_test, X_test.shape[0], X_test.shape[1]), 4)
    return [rmse, r2_train, adj_r2_train, r2_test, adj_r2_test]

# å»ºç«‹ç‰¹å¾µèˆ‡ç›®æ¨™æ¬„ä½
features = df.drop(['ç¸½åƒ¹', 'äº¤æ˜“æ—¥æœŸ', 'ç¶“åº¦', 'ç·¯åº¦', 'æ˜¯å¦æƒ¡æ„æŠ¬åƒ¹'], axis=1).columns
target = 'ç¸½åƒ¹'

# è¨“ç·´/æ¸¬è©¦åˆ‡åˆ†
from sklearn.model_selection import train_test_split
seed = 42
X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=seed)

# æ¨¡å‹åˆ—è¡¨
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures

lst_model, lst_info = [], []

# å¤šå…ƒç·šæ€§è¿´æ­¸
lst_model.append(linear_model.LinearRegression())
lst_info.append(['å¤šå…ƒè¿´æ­¸','15 features'])

# Ridge
lst_model.append(linear_model.Ridge(alpha=10))
lst_info.append(['Ridge','15 features'])

# å¤šé …å¼ç‰¹å¾µ
poly_fea = PolynomialFeatures(degree=2)
X_train_poly = poly_fea.fit_transform(X_train)
X_test_poly = poly_fea.fit_transform(X_test)

# å¤šé …å¼è¿´æ­¸
lst_model.append(linear_model.LinearRegression())
lst_info.append(['å¤šé …å¼è¿´æ­¸','deg=2'])

# å¤šé …å¼ + L1æ­£è¦åŒ–
lst_model.append(linear_model.Lasso(alpha=10))
lst_info.append(['å¤šé …å¼è¿´æ­¸+L1æ­£è¦åŒ–','deg=2'])

# è©•ä¼°æ¯å€‹æ¨¡å‹
idx = df_eval.shape[0]
for i in range(len(lst_model)):
    if 'å¤šé …å¼' in lst_info[i][0]:
        X_train_temp, X_test_temp = X_train_poly, X_test_poly
    else:
        X_train_temp, X_test_temp = X_train, X_test

    model = lst_model[i].fit(X_train_temp, y_train)
    row = lst_info[i] + measurement(model, X_train_temp, X_test_temp)
    df_eval.loc[idx+i] = row

print('å°è¨“ç·´é›†çš„æœ€å¤§ Adjusted R-squared: %.4f' % max(df_eval['adj. R2 (train)']))
print('å°æ¸¬è©¦é›†çš„æœ€å° RMSE:%d' % min(df_eval['RMSE (test)']))
print('å…©å€‹æ¨¡å‹å°æ¸¬è©¦é›†çš„æœ€å¤§ Adjusted R-squared: %.4f' %
      max(df_eval.loc[:1, 'adj. R2 (test)']))

# ================================================
# â— é æ¸¬æˆ¿åƒ¹ + é æ¸¬æ˜¯å¦æƒ¡æ„æŠ¬åƒ¹
# ================================================
X = df[features]
y = df[target]
X_poly = poly_fea.fit_transform(X)

# å»ºç«‹æ–°æ¨£æœ¬ï¼ˆè«‹æ ¹æ“šå¯¦éš›ç‰¹å¾µæ•¸èª¿æ•´ï¼‰
new = np.array([36, 99, 32, 4, 4, 0, 3, 2, 1, 0, 0, 0, 0, 0, 1]).reshape(1, -1)
df_new = pd.DataFrame(new, columns=features)
df_poly_fea = poly_fea.fit_transform(df_new)

# é¸æ“‡æœ€ä½³æ¨¡å‹
lst = df_eval['adj. R2 (test)'].tolist()
idx = lst.index(max(lst))

if idx <= 1:
    model = lst_model[idx].fit(X, y)
    price = model.predict(df_new)[0]
else:
    model = lst_model[idx].fit(X_poly, y)
    price = model.predict(df_poly_fea)[0]

print(f'\nğŸ’° é æ¸¬æˆ¿åƒ¹ï¼š{int(price)} è¬å…ƒ')

# =====================================================
# â›” å»ºç«‹åˆ†é¡æ¨¡å‹ä¾†é æ¸¬æ˜¯å¦æƒ¡æ„æŠ¬åƒ¹
# =====================================================
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

X_class = df[features]
y_class = df['æ˜¯å¦æƒ¡æ„æŠ¬åƒ¹']
Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_class, y_class, test_size=0.2, random_state=seed)

clf = RandomForestClassifier(n_estimators=100, random_state=seed)
clf.fit(Xc_train, yc_train)

# é æ¸¬æ–°æ¨£æœ¬æ˜¯å¦æœ‰æƒ¡æ„æŠ¬åƒ¹
is_overpriced = clf.predict(df_new)[0]
label = 'æœ‰æƒ¡æ„æŠ¬åƒ¹' if is_overpriced else 'åƒ¹æ ¼æ­£å¸¸'
print(f'ğŸ•µï¸ åˆ¤æ–·çµæœï¼š{label}')
